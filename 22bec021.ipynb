{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AI-Based Automated Grading System for Lecture Summaries\n",
        "\n",
        "This notebook implements a fully automated, explainable, and reproducible AI grading system that evaluates student-written summaries by comparing them to lecture transcripts.\n",
        "\n",
        "## Configuration\n",
        "Set your roll number below to generate the correct output filename.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration: Set your roll number here\n",
        "ROLL_NUMBER = \"22bec021\"  # Replace with your actual roll number\n",
        "\n",
        "# Input file paths\n",
        "LECTURE_TRANSCRIPT_PATH = \"C:\\Users\\jallu\\Downloads\\7Aug2025 T+S(Agentic AI)\\Agentic AI Class - 2025_08_07 18_10 GMT+05_30 - Transcript.docx\"\n",
        "STUDENT_SUMMARIES_PATH = \"C:\\Users\\jallu\\Downloads\\7Aug2025 T+S(Agentic AI)\\Attendance + Summary 1.xlsx\"\n",
        "\n",
        "# Output file path\n",
        "OUTPUT_FILE_PATH = f\"Gradedresults{ROLL_NUMBER}.xlsx\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install and Import Required Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages if not already installed\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_package(package, import_name=None):\n",
        "    \"\"\"\n",
        "    Install a package if not already installed.\n",
        "    \n",
        "    Args:\n",
        "        package (str): Package name for pip install\n",
        "        import_name (str): Name to use for import (if different from package name)\n",
        "    \"\"\"\n",
        "    if import_name is None:\n",
        "        # Map package names to their import names\n",
        "        import_map = {\n",
        "            \"python-docx\": \"docx\",\n",
        "            \"scikit-learn\": \"sklearn\"\n",
        "        }\n",
        "        import_name = import_map.get(package, package.replace(\"-\", \"_\"))\n",
        "    \n",
        "    try:\n",
        "        __import__(import_name)\n",
        "    except ImportError:\n",
        "        print(f\"Installing {package}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package], \n",
        "                            stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "# Install required packages\n",
        "packages = [\n",
        "    (\"python-docx\", \"docx\"),\n",
        "    (\"pandas\", \"pandas\"),\n",
        "    (\"openpyxl\", \"openpyxl\"),\n",
        "    (\"sentence-transformers\", \"sentence_transformers\"),\n",
        "    (\"numpy\", \"numpy\"),\n",
        "    (\"scikit-learn\", \"sklearn\")\n",
        "]\n",
        "\n",
        "for package, import_name in packages:\n",
        "    install_package(package, import_name)\n",
        "\n",
        "print(\"All required packages installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all required libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from docx import Document\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Lecture Transcript from DOCX\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_lecture_transcript(docx_path):\n",
        "    \"\"\"\n",
        "    Load and extract text from a DOCX file containing the lecture transcript.\n",
        "    \n",
        "    Args:\n",
        "        docx_path (str): Path to the DOCX file\n",
        "    \n",
        "    Returns:\n",
        "        str: Full text content of the lecture transcript\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(docx_path):\n",
        "            raise FileNotFoundError(f\"Lecture transcript file not found: {docx_path}\")\n",
        "        \n",
        "        doc = Document(docx_path)\n",
        "        full_text = []\n",
        "        \n",
        "        # Extract text from all paragraphs\n",
        "        for paragraph in doc.paragraphs:\n",
        "            if paragraph.text.strip():  # Skip empty paragraphs\n",
        "                full_text.append(paragraph.text.strip())\n",
        "        \n",
        "        # Join all paragraphs with newlines\n",
        "        transcript = \"\\n\".join(full_text)\n",
        "        \n",
        "        # Validate that transcript is not empty\n",
        "        if len(transcript.strip()) < 50:\n",
        "            raise ValueError(\"Lecture transcript appears to be empty or too short. Please check the file.\")\n",
        "        \n",
        "        print(f\"Successfully loaded lecture transcript ({len(transcript)} characters)\")\n",
        "        return transcript\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading lecture transcript: {e}\")\n",
        "        raise\n",
        "\n",
        "# Load the lecture transcript\n",
        "lecture_transcript = load_lecture_transcript(LECTURE_TRANSCRIPT_PATH)\n",
        "print(f\"\\nFirst 500 characters of transcript:\\n{lecture_transcript[:500]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Student Summaries from Excel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_student_summaries(excel_path):\n",
        "    \"\"\"\n",
        "    Load student summaries from an Excel file.\n",
        "    \n",
        "    Args:\n",
        "        excel_path (str): Path to the Excel file\n",
        "    \n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame containing student information and summaries\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(excel_path):\n",
        "            raise FileNotFoundError(f\"Student summaries file not found: {excel_path}\")\n",
        "        \n",
        "        df = pd.read_excel(excel_path)\n",
        "        \n",
        "        # Validate that DataFrame is not empty\n",
        "        if len(df) == 0:\n",
        "            raise ValueError(\"Student summaries file is empty. Please check the file.\")\n",
        "        \n",
        "        # Expected columns: Email Address, Name of the Student, Roll Number, Institute, Summary\n",
        "        print(f\"Loaded {len(df)} student summaries\")\n",
        "        print(f\"\\nColumns found: {list(df.columns)}\")\n",
        "        print(f\"\\nFirst few rows:\")\n",
        "        print(df.head())\n",
        "        \n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading student summaries: {e}\")\n",
        "        raise\n",
        "\n",
        "# Load student summaries\n",
        "student_df = load_student_summaries(STUDENT_SUMMARIES_PATH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Initialize AI Models for Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Sentence Transformer model for semantic similarity\n",
        "# Using a pre-trained model that works well for semantic similarity tasks\n",
        "print(\"Loading Sentence Transformer model...\")\n",
        "print(\"Note: This may take a few minutes on first run as the model downloads (~80MB)\")\n",
        "try:\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight and effective model\n",
        "    print(\"Model loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    print(\"Please ensure you have an internet connection for the first run.\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Extract Key Points from Lecture Transcript\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_key_points(transcript, max_sentences=50):\n",
        "    \"\"\"\n",
        "    Extract key points from the lecture transcript by splitting into sentences\n",
        "    and identifying important sections.\n",
        "    \n",
        "    Args:\n",
        "        transcript (str): Full lecture transcript text\n",
        "        max_sentences (int): Maximum number of sentences to consider\n",
        "    \n",
        "    Returns:\n",
        "        list: List of key sentences/points from the transcript\n",
        "    \"\"\"\n",
        "    # Split transcript into sentences\n",
        "    sentences = transcript.replace('\\n', ' ').split('.')\n",
        "    sentences = [s.strip() for s in sentences if len(s.strip()) > 20]  # Filter very short sentences\n",
        "    \n",
        "    # Take a representative sample of sentences (beginning, middle, end)\n",
        "    if len(sentences) > max_sentences:\n",
        "        # Sample from different parts of the transcript\n",
        "        step = len(sentences) // max_sentences\n",
        "        key_sentences = sentences[::step][:max_sentences]\n",
        "    else:\n",
        "        key_sentences = sentences\n",
        "    \n",
        "    return key_sentences\n",
        "\n",
        "# Extract key points from lecture transcript\n",
        "key_points = extract_key_points(lecture_transcript)\n",
        "print(f\"Extracted {len(key_points)} key points from the lecture transcript\")\n",
        "print(f\"\\nSample key points:\\n{key_points[:3]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluation Function - Hybrid Approach (Embeddings + Rule-Based Scoring)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_semantic_similarity(summary, reference_text, model):\n",
        "    \"\"\"\n",
        "    Calculate semantic similarity between summary and reference text using embeddings.\n",
        "    \n",
        "    Args:\n",
        "        summary (str): Student's summary text\n",
        "        reference_text (str): Reference text (lecture transcript or key points)\n",
        "        model: SentenceTransformer model\n",
        "    \n",
        "    Returns:\n",
        "        float: Similarity score between 0 and 1\n",
        "    \"\"\"\n",
        "    # Truncate very long texts to avoid memory issues (max 5000 chars for reference)\n",
        "    if len(reference_text) > 5000:\n",
        "        reference_text = reference_text[:5000]\n",
        "    \n",
        "    # Truncate very long summaries (max 2000 chars)\n",
        "    if len(summary) > 2000:\n",
        "        summary = summary[:2000]\n",
        "    \n",
        "    # Generate embeddings\n",
        "    summary_embedding = model.encode(summary, convert_to_numpy=True, show_progress_bar=False)\n",
        "    reference_embedding = model.encode(reference_text, convert_to_numpy=True, show_progress_bar=False)\n",
        "    \n",
        "    # Calculate cosine similarity\n",
        "    similarity = cosine_similarity([summary_embedding], [reference_embedding])[0][0]\n",
        "    \n",
        "    return float(similarity)\n",
        "\n",
        "def calculate_coverage_score(summary, key_points, model):\n",
        "    \"\"\"\n",
        "    Calculate how well the summary covers key points from the lecture.\n",
        "    \n",
        "    Args:\n",
        "        summary (str): Student's summary text\n",
        "        key_points (list): List of key points from the lecture\n",
        "        model: SentenceTransformer model\n",
        "    \n",
        "    Returns:\n",
        "        float: Coverage score between 0 and 1\n",
        "    \"\"\"\n",
        "    if not key_points or not summary:\n",
        "        return 0.0\n",
        "    \n",
        "    # Truncate very long summaries (max 2000 chars)\n",
        "    if len(summary) > 2000:\n",
        "        summary = summary[:2000]\n",
        "    \n",
        "    # Calculate similarity between summary and each key point\n",
        "    summary_embedding = model.encode(summary, convert_to_numpy=True, show_progress_bar=False)\n",
        "    \n",
        "    similarities = []\n",
        "    for point in key_points:\n",
        "        if len(point.strip()) > 10:  # Only consider substantial points\n",
        "            # Truncate very long key points\n",
        "            point_text = point[:500] if len(point) > 500 else point\n",
        "            point_embedding = model.encode(point_text, convert_to_numpy=True, show_progress_bar=False)\n",
        "            similarity = cosine_similarity([summary_embedding], [point_embedding])[0][0]\n",
        "            similarities.append(similarity)\n",
        "    \n",
        "    # Coverage is the average of top similarities (how many key points are covered)\n",
        "    if similarities:\n",
        "        # Consider a key point \"covered\" if similarity > 0.3\n",
        "        covered_points = sum(1 for s in similarities if s > 0.3)\n",
        "        coverage_score = covered_points / len(similarities)\n",
        "        return min(coverage_score, 1.0)\n",
        "    \n",
        "    return 0.0\n",
        "\n",
        "def calculate_clarity_score(summary):\n",
        "    \"\"\"\n",
        "    Calculate clarity and coherence score based on text characteristics.\n",
        "    \n",
        "    Args:\n",
        "        summary (str): Student's summary text\n",
        "    \n",
        "    Returns:\n",
        "        float: Clarity score between 0 and 1\n",
        "    \"\"\"\n",
        "    if not summary or len(summary.strip()) < 10:\n",
        "        return 0.0\n",
        "    \n",
        "    # Basic heuristics for clarity\n",
        "    word_count = len(summary.split())\n",
        "    sentence_count = len([s for s in summary.split('.') if s.strip()])\n",
        "    \n",
        "    # Check for reasonable sentence length (not too short, not too long)\n",
        "    avg_sentence_length = word_count / max(sentence_count, 1)\n",
        "    \n",
        "    # Score based on:\n",
        "    # - Has reasonable length (not too short, not too verbose)\n",
        "    # - Has multiple sentences (shows structure)\n",
        "    # - Average sentence length is reasonable (10-25 words is good)\n",
        "    \n",
        "    length_score = min(1.0, word_count / 100)  # Prefer summaries with at least 100 words\n",
        "    structure_score = min(1.0, sentence_count / 5)  # Prefer at least 5 sentences\n",
        "    sentence_quality = 1.0 if 10 <= avg_sentence_length <= 30 else 0.7\n",
        "    \n",
        "    # Combined clarity score\n",
        "    clarity = (length_score * 0.3 + structure_score * 0.3 + sentence_quality * 0.4)\n",
        "    \n",
        "    return min(clarity, 1.0)\n",
        "\n",
        "def check_grammar_basic(summary):\n",
        "    \"\"\"\n",
        "    Basic grammar check using simple heuristics.\n",
        "    \n",
        "    Args:\n",
        "        summary (str): Student's summary text\n",
        "    \n",
        "    Returns:\n",
        "        float: Grammar score between 0 and 1\n",
        "    \"\"\"\n",
        "    if not summary or len(summary.strip()) < 10:\n",
        "        return 0.0\n",
        "    \n",
        "    # Basic checks:\n",
        "    # - Proper capitalization at sentence start\n",
        "    # - Sentence ending punctuation\n",
        "    # - No excessive repeated characters\n",
        "    \n",
        "    sentences = [s.strip() for s in summary.split('.') if s.strip()]\n",
        "    if not sentences:\n",
        "        return 0.0\n",
        "    \n",
        "    proper_caps = sum(1 for s in sentences if s and s[0].isupper()) / len(sentences)\n",
        "    \n",
        "    # Check for excessive repetition (e.g., \"aaaa\")\n",
        "    has_repetition = any(len(set(word)) == 1 and len(word) > 3 for word in summary.split())\n",
        "    repetition_penalty = 0.0 if has_repetition else 1.0\n",
        "    \n",
        "    grammar_score = (proper_caps * 0.7 + repetition_penalty * 0.3)\n",
        "    \n",
        "    return grammar_score\n",
        "\n",
        "print(\"Evaluation functions defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Main Evaluation Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_explanation(score, coverage, similarity, clarity, completeness, grammar, summary, word_count):\n",
        "    \"\"\"\n",
        "    Generate a 2-3 sentence explanation justifying the score.\n",
        "    \n",
        "    Args:\n",
        "        score (float): Final score out of 10\n",
        "        coverage (float): Coverage score\n",
        "        similarity (float): Semantic similarity score\n",
        "        clarity (float): Clarity score\n",
        "        completeness (float): Completeness score\n",
        "        grammar (float): Grammar score\n",
        "        summary (str): The summary text\n",
        "        word_count (int): Word count of summary\n",
        "    \n",
        "    Returns:\n",
        "        str: Explanation text\n",
        "    \"\"\"\n",
        "    strengths = []\n",
        "    weaknesses = []\n",
        "    \n",
        "    # Identify strengths\n",
        "    if coverage > 0.7:\n",
        "        strengths.append(\"covers most key lecture points\")\n",
        "    elif coverage > 0.4:\n",
        "        strengths.append(\"covers some key lecture points\")\n",
        "    \n",
        "    if similarity > 0.6:\n",
        "        strengths.append(\"maintains good semantic alignment with the lecture content\")\n",
        "    \n",
        "    if clarity > 0.7:\n",
        "        strengths.append(\"demonstrates clear and coherent writing\")\n",
        "    \n",
        "    if completeness > 0.7:\n",
        "        strengths.append(\"provides comprehensive coverage of major topics\")\n",
        "    \n",
        "    if grammar > 0.8:\n",
        "        strengths.append(\"shows good grammatical structure\")\n",
        "    \n",
        "    # Identify weaknesses\n",
        "    if coverage < 0.4:\n",
        "        weaknesses.append(\"omits many key lecture points\")\n",
        "    elif coverage < 0.6:\n",
        "        weaknesses.append(\"misses some important lecture points\")\n",
        "    \n",
        "    if similarity < 0.4:\n",
        "        weaknesses.append(\"shows limited alignment with lecture content\")\n",
        "    \n",
        "    if clarity < 0.5:\n",
        "        weaknesses.append(\"lacks clarity and coherent structure\")\n",
        "    \n",
        "    if word_count < 50:\n",
        "        weaknesses.append(\"is too brief and lacks detail\")\n",
        "    elif word_count > 500:\n",
        "        weaknesses.append(\"is overly verbose and could be more concise\")\n",
        "    \n",
        "    if grammar < 0.6:\n",
        "        weaknesses.append(\"contains grammatical issues\")\n",
        "    \n",
        "    # Construct explanation\n",
        "    explanation_parts = []\n",
        "    \n",
        "    if strengths:\n",
        "        explanation_parts.append(f\"The summary {', '.join(strengths[:2])}.\")\n",
        "    \n",
        "    if weaknesses:\n",
        "        explanation_parts.append(f\"However, it {', '.join(weaknesses[:2])}.\")\n",
        "    \n",
        "    if not explanation_parts:\n",
        "        if score >= 7:\n",
        "            explanation_parts.append(\"The summary provides a solid overview of the lecture with good coverage and clarity.\")\n",
        "        elif score >= 4:\n",
        "            explanation_parts.append(\"The summary demonstrates moderate understanding but could benefit from more comprehensive coverage.\")\n",
        "        else:\n",
        "            explanation_parts.append(\"The summary requires significant improvement in coverage, clarity, and alignment with lecture content.\")\n",
        "    \n",
        "    # Add score context\n",
        "    if score >= 8:\n",
        "        explanation_parts.append(\"Overall, this represents a strong summary that effectively captures the lecture's main points.\")\n",
        "    elif score >= 6:\n",
        "        explanation_parts.append(\"The summary meets basic requirements but has room for improvement in depth and completeness.\")\n",
        "    elif score >= 4:\n",
        "        explanation_parts.append(\"The summary shows partial understanding but lacks sufficient detail and coverage.\")\n",
        "    else:\n",
        "        explanation_parts.append(\"The summary needs substantial revision to adequately represent the lecture content.\")\n",
        "    \n",
        "    return \" \".join(explanation_parts)\n",
        "\n",
        "def evaluate_summary(summary, lecture_transcript, key_points, model):\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation of a student summary against the lecture transcript.\n",
        "    \n",
        "    Args:\n",
        "        summary (str): Student's summary text\n",
        "        lecture_transcript (str): Full lecture transcript\n",
        "        key_points (list): List of key points from the lecture\n",
        "        model: SentenceTransformer model for embeddings\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (score out of 10, explanation string)\n",
        "    \"\"\"\n",
        "    if not summary or len(summary.strip()) < 10:\n",
        "        return 0.0, \"Summary is too short or empty. No meaningful content provided.\"\n",
        "    \n",
        "    # Calculate individual component scores (all between 0 and 1)\n",
        "    \n",
        "    # 1. Semantic Similarity (how well summary matches overall transcript)\n",
        "    semantic_sim = calculate_semantic_similarity(summary, lecture_transcript[:5000], model)  # Use first 5000 chars for efficiency\n",
        "    \n",
        "    # 2. Coverage of main lecture points\n",
        "    coverage_score = calculate_coverage_score(summary, key_points, model)\n",
        "    \n",
        "    # 3. Clarity and coherence\n",
        "    clarity_score = calculate_clarity_score(summary)\n",
        "    \n",
        "    # 4. Grammar (basic check)\n",
        "    grammar_score = check_grammar_basic(summary)\n",
        "    \n",
        "    # 5. Completeness (based on length and coverage)\n",
        "    word_count = len(summary.split())\n",
        "    completeness_score = min(1.0, (coverage_score * 0.7 + min(1.0, word_count / 200) * 0.3))\n",
        "    \n",
        "    # Weighted combination of scores\n",
        "    # Coverage: 30%, Semantic Similarity: 25%, Clarity: 20%, Completeness: 15%, Grammar: 10%\n",
        "    final_score = (\n",
        "        coverage_score * 0.30 +\n",
        "        semantic_sim * 0.25 +\n",
        "        clarity_score * 0.20 +\n",
        "        completeness_score * 0.15 +\n",
        "        grammar_score * 0.10\n",
        "    )\n",
        "    \n",
        "    # Convert to 0-10 scale\n",
        "    score_0_10 = final_score * 10\n",
        "    \n",
        "    # Generate explanation\n",
        "    explanation = generate_explanation(\n",
        "        score_0_10, coverage_score, semantic_sim, clarity_score, \n",
        "        completeness_score, grammar_score, summary, word_count\n",
        "    )\n",
        "    \n",
        "    return round(score_0_10, 2), explanation\n",
        "\n",
        "print(\"Main evaluation function defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Process All Student Summaries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing student 82/82...\n",
            "\n",
            "Completed processing all 82 summaries!\n"
          ]
        }
      ],
      "source": [
        "def process_all_summaries(student_df, lecture_transcript, key_points, model):\n",
        "    \"\"\"\n",
        "    Process all student summaries and generate scores and explanations.\n",
        "    \n",
        "    Args:\n",
        "        student_df (pd.DataFrame): DataFrame with student information and summaries\n",
        "        lecture_transcript (str): Full lecture transcript\n",
        "        key_points (list): List of key points from the lecture\n",
        "        model: SentenceTransformer model\n",
        "    \n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with all original columns plus Score and Explanation\n",
        "    \"\"\"\n",
        "    # Validate inputs\n",
        "    if student_df is None or len(student_df) == 0:\n",
        "        raise ValueError(\"Student DataFrame is empty or None\")\n",
        "    \n",
        "    if not lecture_transcript or len(lecture_transcript.strip()) < 50:\n",
        "        raise ValueError(\"Lecture transcript is empty or too short\")\n",
        "    \n",
        "    if not key_points or len(key_points) == 0:\n",
        "        raise ValueError(\"No key points extracted from lecture transcript\")\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    # Determine the summary column name (handle variations)\n",
        "    summary_col = None\n",
        "    possible_names = ['Summary', 'summary', 'Summary Text', 'Student Summary']\n",
        "    for name in possible_names:\n",
        "        if name in student_df.columns:\n",
        "            summary_col = name\n",
        "            break\n",
        "    \n",
        "    if summary_col is None:\n",
        "        # Try to find a column that might contain summaries\n",
        "        for col in student_df.columns:\n",
        "            if 'summary' in col.lower():\n",
        "                summary_col = col\n",
        "                break\n",
        "    \n",
        "    if summary_col is None:\n",
        "        raise ValueError(f\"Could not find summary column. Available columns: {list(student_df.columns)}\")\n",
        "    \n",
        "    print(f\"Using column '{summary_col}' for summaries\")\n",
        "    print(f\"\\nProcessing {len(student_df)} summaries...\")\n",
        "    \n",
        "    for idx, row in student_df.iterrows():\n",
        "        # Handle NaN and None values\n",
        "        summary = str(row[summary_col]) if pd.notna(row[summary_col]) else \"\"\n",
        "        summary = summary.strip() if summary else \"\"\n",
        "        \n",
        "        print(f\"Processing student {idx + 1}/{len(student_df)}...\", end=\"\\r\")\n",
        "        \n",
        "        try:\n",
        "            # Evaluate the summary\n",
        "            score, explanation = evaluate_summary(summary, lecture_transcript, key_points, model)\n",
        "        except Exception as e:\n",
        "            print(f\"\\nWarning: Error evaluating summary for student {idx + 1}: {e}\")\n",
        "            score = 0.0\n",
        "            explanation = f\"Error during evaluation: {str(e)}\"\n",
        "        \n",
        "        # Create result row - preserve all original columns\n",
        "        result_row = row.to_dict()\n",
        "        result_row['Numerical Score'] = score\n",
        "        result_row['Short explanation'] = explanation\n",
        "        \n",
        "        results.append(result_row)\n",
        "    \n",
        "    print(f\"\\n\\nCompleted processing all {len(student_df)} summaries!\")\n",
        "    \n",
        "    # Create results DataFrame\n",
        "    if not results:\n",
        "        raise ValueError(\"No results generated. Please check your input data.\")\n",
        "    \n",
        "    results_df = pd.DataFrame(results)\n",
        "    \n",
        "    return results_df\n",
        "\n",
        "# Process all summaries\n",
        "print(\"Starting batch processing of all student summaries...\")\n",
        "graded_results = process_all_summaries(student_df, lecture_transcript, key_points, model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Prepare Output DataFrame with Required Columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output DataFrame prepared!\n",
            "\n",
            "Columns: ['Email Address', 'Name', 'Roll Number', 'Institute', 'Original Summary', 'Numerical Score', 'Short explanation']\n",
            "\n",
            "Shape: (82, 7)\n",
            "\n",
            "First few rows:\n",
            "            Email Address                    Name Roll Number     Institute  \\\n",
            "0   cs22b1027@iiitr.ac.in                 Deepthi              IIIT Raichur   \n",
            "1  23bds014@iiitdwd.ac.in            Bongu Ashish              IIIT Dharwad   \n",
            "2   cs22b1036@iiitr.ac.in  Mudavath Srinivas Naik              IIIT Raichur   \n",
            "3   cs22b1007@iiitr.ac.in          Aman Chaurasia              IIIT Raichur   \n",
            "4   cs22b1032@iiitr.ac.in            Krishu Patel              IIIT Raichur   \n",
            "\n",
            "                                    Original Summary  Numerical Score  \\\n",
            "0  Learned about neural networks in AI generative...             2.44   \n",
            "1                                         Insightful             2.03   \n",
            "2  In todayâ€™s class, I learned about the Med Agen...             3.71   \n",
            "3  In this class, I learned the basics to advance...             4.27   \n",
            "4  overview of the language model. Input as the o...             3.48   \n",
            "\n",
            "                                   Short explanation  \n",
            "0  The summary shows good grammatical structure. ...  \n",
            "1  The summary shows good grammatical structure. ...  \n",
            "2  The summary demonstrates clear and coherent wr...  \n",
            "3  The summary demonstrates clear and coherent wr...  \n",
            "4  The summary demonstrates clear and coherent wr...  \n"
          ]
        }
      ],
      "source": [
        "def prepare_output_dataframe(graded_results):\n",
        "    \"\"\"\n",
        "    Prepare the final output DataFrame with all required columns in the correct order.\n",
        "    \n",
        "    Args:\n",
        "        graded_results (pd.DataFrame): DataFrame with graded results\n",
        "    \n",
        "    Returns:\n",
        "        pd.DataFrame: Formatted output DataFrame\n",
        "    \"\"\"\n",
        "    # Validate that required columns exist\n",
        "    if 'Numerical Score' not in graded_results.columns:\n",
        "        raise ValueError(\"Missing 'Numerical Score' column in graded results\")\n",
        "    if 'Short explanation' not in graded_results.columns:\n",
        "        raise ValueError(\"Missing 'Short explanation' column in graded results\")\n",
        "    \n",
        "    # Map column names (handle variations)\n",
        "    column_mapping = {\n",
        "        'Email Address': ['Email Address', 'Email', 'email', 'Email address', 'Email Address'],\n",
        "        'Name': ['Name of the Student', 'Name', 'Student Name', 'name', 'Name of Student'],\n",
        "        'Roll Number': ['Roll Number', 'Roll No', 'Roll', 'roll number', 'RollNumber'],\n",
        "        'Institute': ['Institute', 'institute', 'Institution', 'Institution Name'],\n",
        "        'Original Summary': None,  # Will be determined from summary column\n",
        "        'Numerical Score': 'Numerical Score',\n",
        "        'Short explanation': 'Short explanation'\n",
        "    }\n",
        "    \n",
        "    output_data = {}\n",
        "    \n",
        "    # Find and map columns\n",
        "    for target_col, possible_names in column_mapping.items():\n",
        "        if possible_names is None:\n",
        "            # Special handling for Original Summary\n",
        "            summary_col = None\n",
        "            for col in graded_results.columns:\n",
        "                if 'summary' in col.lower() and col not in ['Short explanation', 'Short Explanation']:\n",
        "                    summary_col = col\n",
        "                    break\n",
        "            if summary_col:\n",
        "                output_data[target_col] = graded_results[summary_col].fillna(\"\")\n",
        "            else:\n",
        "                # Create empty column if summary column not found\n",
        "                output_data[target_col] = [\"\"] * len(graded_results)\n",
        "        else:\n",
        "            found = False\n",
        "            for possible_name in possible_names:\n",
        "                if possible_name in graded_results.columns:\n",
        "                    # Fill NaN values with empty string\n",
        "                    output_data[target_col] = graded_results[possible_name].fillna(\"\")\n",
        "                    found = True\n",
        "                    break\n",
        "            if not found:\n",
        "                # If column not found, create empty column\n",
        "                output_data[target_col] = [\"\"] * len(graded_results)\n",
        "    \n",
        "    # Ensure all columns have the same length\n",
        "    expected_length = len(graded_results)\n",
        "    for key, value in output_data.items():\n",
        "        if isinstance(value, pd.Series):\n",
        "            if len(value) != expected_length:\n",
        "                raise ValueError(f\"Column '{key}' has incorrect length: {len(value)} != {expected_length}\")\n",
        "        elif isinstance(value, list):\n",
        "            if len(value) != expected_length:\n",
        "                output_data[key] = [\"\"] * expected_length\n",
        "    \n",
        "    # Create output DataFrame with required columns in order\n",
        "    output_df = pd.DataFrame({\n",
        "        'Email Address': output_data.get('Email Address', [\"\"] * expected_length),\n",
        "        'Name': output_data.get('Name', [\"\"] * expected_length),\n",
        "        'Roll Number': output_data.get('Roll Number', [\"\"] * expected_length),\n",
        "        'Institute': output_data.get('Institute', [\"\"] * expected_length),\n",
        "        'Original Summary': output_data.get('Original Summary', [\"\"] * expected_length),\n",
        "        'Numerical Score': graded_results['Numerical Score'],\n",
        "        'Short explanation': graded_results['Short explanation']\n",
        "    })\n",
        "    \n",
        "    # Validate output DataFrame\n",
        "    if len(output_df) == 0:\n",
        "        raise ValueError(\"Output DataFrame is empty\")\n",
        "    \n",
        "    return output_df\n",
        "\n",
        "# Prepare output DataFrame\n",
        "output_df = prepare_output_dataframe(graded_results)\n",
        "\n",
        "print(\"Output DataFrame prepared!\")\n",
        "print(f\"\\nColumns: {list(output_df.columns)}\")\n",
        "print(f\"\\nShape: {output_df.shape}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "print(output_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Export Results to Excel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Successfully exported results to Gradedresults22bec021.xlsx\n",
            "Total students graded: 82\n",
            "\n",
            "Score statistics:\n",
            "  Mean: 3.83\n",
            "  Min: 2.03\n",
            "  Max: 4.81\n",
            "  Std: 0.59\n",
            "  File size: 23.05 KB\n",
            "\n",
            "==================================================\n",
            "GRADING COMPLETE!\n",
            "==================================================\n",
            "Output file: Gradedresults22bec021.xlsx\n",
            "File location: c:\\Users\\jallu\\OneDrive\\Desktop\\Quiz\\Gradedresults22bec021.xlsx\n"
          ]
        }
      ],
      "source": [
        "def export_to_excel(output_df, output_path):\n",
        "    \"\"\"\n",
        "    Export the graded results to an Excel file.\n",
        "    \n",
        "    Args:\n",
        "        output_df (pd.DataFrame): DataFrame with graded results\n",
        "        output_path (str): Path to output Excel file\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Validate DataFrame before export\n",
        "        if output_df is None or len(output_df) == 0:\n",
        "            raise ValueError(\"Cannot export empty DataFrame\")\n",
        "        \n",
        "        # Check if required columns exist\n",
        "        required_cols = ['Email Address', 'Name', 'Roll Number', 'Institute', \n",
        "                        'Original Summary', 'Numerical Score', 'Short explanation']\n",
        "        missing_cols = [col for col in required_cols if col not in output_df.columns]\n",
        "        if missing_cols:\n",
        "            raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
        "        \n",
        "        # Ensure output directory exists\n",
        "        output_dir = os.path.dirname(output_path) if os.path.dirname(output_path) else \".\"\n",
        "        if output_dir and not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "        \n",
        "        # Export to Excel\n",
        "        output_df.to_excel(output_path, index=False, engine='openpyxl')\n",
        "        \n",
        "        # Verify file was created\n",
        "        if not os.path.exists(output_path):\n",
        "            raise FileNotFoundError(f\"Output file was not created: {output_path}\")\n",
        "        \n",
        "        print(f\"\\nSuccessfully exported results to {output_path}\")\n",
        "        print(f\"Total students graded: {len(output_df)}\")\n",
        "        print(f\"\\nScore statistics:\")\n",
        "        print(f\"  Mean: {output_df['Numerical Score'].mean():.2f}\")\n",
        "        print(f\"  Min: {output_df['Numerical Score'].min():.2f}\")\n",
        "        print(f\"  Max: {output_df['Numerical Score'].max():.2f}\")\n",
        "        print(f\"  Std: {output_df['Numerical Score'].std():.2f}\")\n",
        "        \n",
        "        # Show file size\n",
        "        file_size = os.path.getsize(output_path) / 1024  # KB\n",
        "        print(f\"  File size: {file_size:.2f} KB\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error exporting to Excel: {e}\")\n",
        "        raise\n",
        "\n",
        "# Export results\n",
        "export_to_excel(output_df, OUTPUT_FILE_PATH)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"GRADING COMPLETE!\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Output file: {OUTPUT_FILE_PATH}\")\n",
        "print(f\"File location: {os.path.abspath(OUTPUT_FILE_PATH)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook implements a fully automated AI-based grading system that:\n",
        "\n",
        "1. **Loads** the lecture transcript from DOCX format\n",
        "2. **Loads** student summaries from Excel format\n",
        "3. **Evaluates** each summary using a hybrid approach:\n",
        "   - Semantic similarity using Sentence Transformers\n",
        "   - Coverage analysis of key lecture points\n",
        "   - Clarity and coherence assessment\n",
        "   - Completeness evaluation\n",
        "   - Basic grammar checking\n",
        "4. **Generates** scores (0-10) with detailed explanations\n",
        "5. **Exports** results to Excel with all required columns\n",
        "\n",
        "The system is fully automated, explainable, and reproducible. All evaluation logic is contained within this notebook.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
